\documentclass[landscape]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{soul}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{tabularx}
\usepackage[margin=0.25in]{geometry}
\begin{document}
\section{Introduction}
The point of this document is to derive norm-optimal point-to-point learning filters based on a performance index that considers multiple future iterations.  These learning filters will then produce control sequences for multiple future iterations.  We can then use the first of these sequences to run the next iteration.

\section{Iteration-Domain ``Lifted'' Model}
This section derives the model that can be used to relate the control sequence over multiple future iterations to the state and error sequences over those iterations.  Suppose that we are given a lifted system model of the form
\begin{align}
x_{j+1} 
& = x_j + G_j(u_{j+1} - u_j) + F_j \left(x^0_{j+1} - x^0_j\right)\\
& = x_j + G_j(u_{j+1} - u_j) + F_j \left(x^F_{j} - x^0_j\right)\\
& = x_j + G_j(u_{j+1} - u_j) + F_j \left(E_F-E_I\right)x_j\\
& = x_j + G_j(u_{j+1} - u_j) + F_j E_F x_j - F_j E_I x_j\\
\end{align}
Then our prediction of the state sequence at $j+2$ is
\begin{align}
x_{j+2} 
& = x_j + G_j(u_{j+2}-u_j) + F_j \left(x^0_{j+2}-x^0_{j}\right)\\
& = x_j + G_j(u_{j+2}-u_j) + F_j \left(E_F x_{j+1}-E_I x_{j}\right)\\
& = x_j + G_j(u_{j+2}-u_j) + F_j E_F x_{j+1} - F_j E_I x_{j}\\
& = x_j + G_j(u_{j+2}-u_j) - F_j E_I x_{j}   + F_j E_F x_{j+1}\\
& = x_j + G_j(u_{j+2}-u_j) - F_j E_I x_{j}   + F_j E_F \left(x_j + G_j(u_{j+1} - u_j) + F_j E_F x_j - F_j E_I x_j\right)\\
& = x_j + G_j(u_{j+2}-u_j) - F_j E_I x_{j}   + F_j E_F x_j + F_j E_F G_j(u_{j+1} - u_j) + F_j E_F F_j E_F x_j - F_j E_F F_j E_I x_j\\
& = x_j + G_j(u_{j+2}-u_j) + F_j E_F G_j(u_{j+1} - u_j) - F_j E_I x_{j}   + F_j E_F x_j + F_j E_F F_j E_F x_j - F_j E_F F_j E_I x_j\\
& = x_j + G_j(u_{j+2}-u_j) + F_j E_F G_j(u_{j+1} - u_j) + F_j \left( E_F -E_I \right) x_{j} + F_j E_F F_j E_F x_j - F_j E_F F_j E_I x_j\\
& = x_j + G_j(u_{j+2}-u_j) + F_j E_F G_j(u_{j+1} - u_j) + F_j \left( E_F -E_I \right) x_{j} + F_j E_F F_j \left( E_F - E_I \right) x_j
\end{align}
The prediction of the state sequence at iteration $j+3$ is
\begin{align}
x_{j+3} 
& = x_j + G_j (u_{j+3} - u_j) + F_j \left(x^0_{j+3} - x^0_{j}\right)\\
& = x_j + G_j (u_{j+3} - u_j) + F_j \left(E_F x_{j+2} - E_I x_{j}\right)\\
& = x_j + G_j (u_{j+3} - u_j) + F_j E_F x_{j+2} - F_j E_I x_{j}\\
& = x_j + G_j (u_{j+3} - u_j) - F_j E_I x_{j} + F_j E_F x_{j+2} \\
& = x_j + G_j (u_{j+3} - u_j) - F_j E_I x_{j} + F_j E_F \left(x_j + G_j(u_{j+2}-u_j) + F_j E_F G_j(u_{j+1} - u_j) + F_j \left( E_F -E_I \right) x_{j} + F_j E_F F_j \left( E_F - E_I \right) x_j \right) \\
& = x_j + G_j (u_{j+3} - u_j) - F_j E_I x_{j} + F_j E_F x_j + F_j E_F G_j(u_{j+2}-u_j) + F_j E_F F_j E_F G_j(u_{j+1} - u_j) + F_j E_F F_j \left( E_F -E_I \right) x_{j} + F_j E_F F_j E_F F_j \left( E_F - E_I \right) x_j  \\
& = x_j + G_j (u_{j+3} - u_j) + F_j E_F G_j(u_{j+2}-u_j) + F_j E_F F_j E_F G_j(u_{j+1} - u_j) + F_j (E_F-E_I) x_{j} + F_j E_F F_j \left( E_F -E_I \right) x_{j} + F_j E_F F_j E_F F_j \left( E_F - E_I \right) x_j  \\
\end{align}
So to summarize, the three expressions are
\begin{align}
x_{j+1} & = x_j 					 &  							 &                  + G_j(u_{j+1} - u_j)& + F_j \left( E_F- E_I \right)& x_j\\ 
x_{j+2} & = x_j 				     &             + G_j(u_{j+2}-u_j)&	        + F_j E_F G_j(u_{j+1} - u_j)& + F_j \left( E_F -E_I \right)& x_{j} + F_j E_F F_j \left( E_F - E_I \right) x_j\\
x_{j+3} & = x_j + G_j (u_{j+3} - u_j)& 	   + F_j E_F G_j(u_{j+2}-u_j)& 	+ F_j E_F F_j E_F G_j(u_{j+1} - u_j)& + F_j \left( E_F - E_I\right)& x_{j} + F_j E_F F_j \left( F_F - E_I \right) x_{j} + F_j E_F F_j E_F F_j \left( E_F -E_I \right) x_j 
\end{align}

Therefore, if we form the uber-lyfted vectors
\begin{align}
\mathbf{x}_{j+1} \triangleq \begin{bmatrix} x_{j+1} \\ x_{j+2} \\ \vdots \\ x_{j+N-1} \\ x_{j+N}\end{bmatrix}, \quad 
\mathbf{u}_{j+1} \triangleq \begin{bmatrix} u_{j+1} \\ u_{j+2} \\ \vdots \\ u_{j+N-1} \\ u_{j+N}\end{bmatrix}
\end{align}
then we can write an expression for $\mathbf{x}_{j+1}$ in terms of $\mathbf{u}_{j+1}$
\begin{align}
\mathbf{x}_{j+1} 
& = \mathbf{I}_x x_j + \begin{bmatrix} \mathbb{I} \\ \mathbb{I} + F_j E_F \\ \mathbb{I} + F_j E_F + F_j E_F F_j E_F\\ \vdots \\ \mathbb{I} + \sum_{k = 1}^{N_i-1} \prod_{m=1}^{k}F_j E_F \\ \mathbb{I} + \sum_{k = 1}^{N_i} \prod_{m=1}^{k}F_j E_F \end{bmatrix} F_j \left(E_F - E_I\right)x_j
+\begin{bmatrix} 
G_j & \mathbb{0} & \mathbb{0} & \hdots & \mathbb{0} \\
F_j E_F G_j   & G_j & \mathbb{0} & \hdots & \mathbb{0} \\
F_j E_F F_j E_F G_j   & F_j E_F G_j & G_j & \hdots & \mathbb{0} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\left(\prod_{m=1}^{N_i-1}F_j E_F\right)G_j & \left(\prod_{m=1}^{N_i-2}F_j E_F\right)G_j & \hdots & G_j & \mathbb{0} \\
\left(\prod_{m=1}^{N_i}F_j E_F\right)G_j & \left(\prod_{m=1}^{N_i-1}F_j E_F\right)G_j & \hdots & F_j E_F G_j & G_j
\end{bmatrix}
\left(\mathbf{u}_{j+1}- \mathbf{I}_u u_j \right)
\end{align}
Here, $\mathbf{I}_x\triangleq \begin{bmatrix} \mathbb{I} & \hdots & \mathbb{I} \end{bmatrix}^T$ and $\mathbf{I}_u$ is defined similarly.  If we define $\mathbf{F}_j$ to be the first matrix, and $\mathbf{G}_j$ to be the second matrix, then our system model as lifted in the iteration domain is
\begin{align}
\mathbf{x}_{j+1} = \left(\mathbf{I}_x +\mathbf{F}_j \right)x_j + \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right)
\end{align}
Next, form the uber-lyfted vector $\mathbf{r} \triangleq \mathbf{I}_r r$, then the uber-lyfted error sequence $\mathbf{e}_{j+1}$ is
\begin{align}
\mathbf{e}_{j+1} 
& = \mathbf{I}_r r - \mathbf{x}_{j+1}\\
& = \mathbf{I}_r r - \left(\mathbf{I}_x +\mathbf{F}_j \right)x_j - \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right)\\
& = \mathbf{I}_r r - \mathbf{I}_x x_j - \mathbf{F}_j x_j - \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right)\\
& = \mathbf{I}_e e_j - \mathbf{F}_j x_j - \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right)
\end{align}
Note that $\mathbf{I}_e=\mathbf{I}_x=\mathbf{I}_r$.

\section{Optimal Learning Filters}
Now, we want to form a performance index that includes terms for 7 things 
\begin{itemize}
	\item a penalty on the size of the control input for each future iteration
	\item a penalty on the deviation in the control input for each future iteration
	\item a penalty on the size of the state for each future iteration
	\item a penalty on the deviation in the state for each future iteration
	\item a penalty on the size of the error for each future iteration
	\item a penalty on the deviation in the error for each future iteration
	\item an economic incentive on the state for each future iteration
\end{itemize}

We can write this as
\begin{align}
\mathbf{J}_{j+N} 
= \sum_{k=1}^{N}(
&   u_{j+k}^T Q_u u_{j+k} + \left(u_{j+k}-u_{j+k-1}\right)^T Q_{\delta u} \left(u_{j+k}-u_{j+k-1}\right) \\
& + x_{j+k}^T Q_x x_{j+k} + \left(x_{j+k}-x_{j+k-1}\right)^T Q_{\delta x} \left(x_{j+k}-x_{j+k-1}\right) \\
& + e_{j+k}^T Q_e e_{j+k} + \left(e_{j+k}-e_{j+k-1}\right)^T Q_{\delta e} \left(e_{j+k}-e_{j+k-1}\right) \\
& + S_x x_{j+k})
\end{align}
Rearranging this to get the $\delta$ terms in their own summation
\begin{align}
\mathbf{J}_{j+N} 
= \sum_{k=1}^{N}(
&   u_{j+k}^T Q_u u_{j+k} 
  + x_{j+k}^T Q_x x_{j+k} 
  + e_{j+k}^T Q_e e_{j+k}
  + S_x x_{j+k}) +\\
&\sum_{k=1}^{N}(
  \left(u_{j+k}-u_{j+k-1}\right)^T Q_{\delta u} \left(u_{j+k}-u_{j+k-1}\right) \\
& + \left(x_{j+k}-x_{j+k-1}\right)^T Q_{\delta x} \left(x_{j+k}-x_{j+k-1}\right) \\
& + \left(e_{j+k}-e_{j+k-1}\right)^T Q_{\delta e} \left(e_{j+k}-e_{j+k-1}\right) 
)
\end{align}
Now, pulling out the terms that include deviation between $j$ and $j+1$,
\begin{align}
\mathbf{J}_{j+N} 
= \sum_{k=1}^{N}(
&   u_{j+k}^T Q_u u_{j+k} 
+ x_{j+k}^T Q_x x_{j+k} 
+ e_{j+k}^T Q_e e_{j+k}
+ S_x x_{j+k}) +\\
&\sum_{k=2}^{N}(
\left(u_{j+k}-u_{j+k-1}\right)^T Q_{\delta u} \left(u_{j+k}-u_{j+k-1}\right) \\
& + \left(x_{j+k}-x_{j+k-1}\right)^T Q_{\delta x} \left(x_{j+k}-x_{j+k-1}\right) \\
& + \left(e_{j+k}-e_{j+k-1}\right)^T Q_{\delta e} \left(e_{j+k}-e_{j+k-1}\right) 
)\\
&+
(u_{j+1}-u_{j})^T Q_{\delta u} (u_{j+1}-u_{j})+
(x_{j+1}-x_{j})^T Q_{\delta x} (x_{j+1}-x_{j})+
(e_{j+1}-e_{j})^T Q_{\delta e} (e_{j+1}-e_{j})
\end{align}
This has an equivalent block form
\begin{equation}
\begin{split}
\mathbf{J}_{j+N} = 
& \mathbf{u}_{j+1}^T \mathbf{Q_u} \mathbf{u}_{j+1} + \mathbf{u}_{j+1}^T \mathbf{D}_u^T \mathbf{Q_{\delta u}} \mathbf{D}_u \mathbf{u}_{j+1} + \left(u_{j+1}-u_j\right)^T Q_{\delta u} \left(u_{j+1}-u_j\right)\\
&+\mathbf{x}_{j+1}^T \mathbf{Q_x} \mathbf{x}_{j+1} + \mathbf{x}_{j+1}^T \mathbf{D}_x^T \mathbf{Q_{\delta x}} \mathbf{D}_x \mathbf{x}_{j+1} + \left(x_{j+1}-x_j\right)^T Q_{\delta x} \left(x_{j+1}-x_j\right)\\
&+\mathbf{e}_{j+1}^T \mathbf{Q_e} \mathbf{e}_{j+1} + \mathbf{e}_{j+1}^T \mathbf{D}_e^T \mathbf{Q_{\delta e}} \mathbf{D}_e \mathbf{e}_{j+1} + \left(e_{j+1}-e_j\right)^T Q_{\delta e} \left(e_{j+1}-e_j\right)\\
&+\mathbf{S_x}\mathbf{x}_{j+1}
\end{split}
\end{equation}
Re-writing $u_{j+1}$ in terms of $\mathbf{u}_{j+1}$ gives
\begin{equation}
\begin{split}
\mathbf{J}_{j+N} = 
& \mathbf{u}_{j+1}^T \mathbf{Q_u} \mathbf{u}_{j+1} + \mathbf{u}_{j+1}^T \mathbf{D}_u^T \mathbf{Q_{\delta u}} \mathbf{D}_u \mathbf{u}_{j+1} 
+ \left(\mathbf{E}_u\mathbf{u}_{j+1}-u_j\right)^T Q_{\delta u} \left(\mathbf{E}_u\mathbf{u}_{j+1}-u_j\right)\\
&+\mathbf{x}_{j+1}^T \mathbf{Q_x} \mathbf{x}_{j+1} + \mathbf{x}_{j+1}^T \mathbf{D}_x^T \mathbf{Q_{\delta x}} \mathbf{D}_x \mathbf{x}_{j+1} 
+ \left(\mathbf{E}_x\mathbf{x}_{j+1}-x_j\right)^T Q_{\delta x} \left(\mathbf{E}_x\mathbf{x}_{j+1}-x_j\right)\\
&+\mathbf{e}_{j+1}^T \mathbf{Q_e} \mathbf{e}_{j+1} + \mathbf{e}_{j+1}^T \mathbf{D}_e^T \mathbf{Q_{\delta e}} \mathbf{D}_e \mathbf{e}_{j+1} 
+ \left(\mathbf{E}_e\mathbf{e}_{j+1}-e_j\right)^T Q_{\delta e} \left(\mathbf{E}_e\mathbf{e}_{j+1}-e_j\right)\\
&+\mathbf{S_x}\mathbf{x}_{j+1}
\end{split}
\end{equation}
Multiplying out the $\delta$ terms at the end of each line and rearranging things gives
\begin{align}
\mathbf{J}_{j+N} 
& = \mathbf{u}_{j+1}^T \left(\mathbf{Q_u} + \mathbf{D}_u^T\mathbf{Q_{\delta u}} \mathbf{D}_u + \mathbf{E}_u^T Q_{\delta u} \mathbf{E}_u \right) \mathbf{u}_{j+1} - 2u_j^T Q_{\delta u} \mathbf{E}_u \mathbf{u}_{j+1} + u_j^T Q_{\delta u} u_j\\
& + \mathbf{x}_{j+1}^T \left(\mathbf{Q_x} + \mathbf{D}_x^T\mathbf{Q_{\delta x}} \mathbf{D}_x + \mathbf{E}_x^T Q_{\delta x} \mathbf{E}_x \right) \mathbf{x}_{j+1} - 2x_j^T Q_{\delta x} \mathbf{E}_x \mathbf{x}_{j+1} + x_j^T Q_{\delta x} x_j\\
& + \mathbf{e}_{j+1}^T \left(\mathbf{Q_e} + \mathbf{D}_e^T\mathbf{Q_{\delta e}} \mathbf{D}_e + \mathbf{E}_e^T Q_{\delta e} \mathbf{E}_e \right) \mathbf{e}_{j+1} - 2u_j^T Q_{\delta e} \mathbf{E}_e \mathbf{e}_{j+1} + e_j^T Q_{\delta e} u e_j\\
& + \mathbf{S_x}\mathbf{x}_{j+1}
\end{align}
Since we're going to differentiate, I'm going to drop the terms that don't depend on $\mathbf{u}_{j+1}$
\begin{align}
\mathbf{J}_{j+N} 
& = \mathbf{u}_{j+1}^T \left(\mathbf{Q_u} + \mathbf{D}_u^T\mathbf{Q_{\delta u}} \mathbf{D}_u + \mathbf{E}_u^T Q_{\delta u} \mathbf{E}_u \right) \mathbf{u}_{j+1} - 2u_j^T Q_{\delta u} \mathbf{E}_u \mathbf{u}_{j+1} \\
& + \mathbf{x}_{j+1}^T \left(\mathbf{Q_x} + \mathbf{D}_x^T\mathbf{Q_{\delta x}} \mathbf{D}_x + \mathbf{E}_x^T Q_{\delta x} \mathbf{E}_x \right) \mathbf{x}_{j+1} - 2x_j^T Q_{\delta x} \mathbf{E}_x \mathbf{x}_{j+1} \\
& + \mathbf{e}_{j+1}^T \left(\mathbf{Q_e} + \mathbf{D}_e^T\mathbf{Q_{\delta e}} \mathbf{D}_e + \mathbf{E}_e^T Q_{\delta e} \mathbf{E}_e \right) \mathbf{e}_{j+1} - 2u_j^T Q_{\delta e} \mathbf{E}_e \mathbf{e}_{j+1} \\
& + \mathbf{S_x}\mathbf{x}_{j+1}
\end{align}
So then the final performance index is
\begin{equation}
\mathbf{J}_{j+N} = 
 \mathbf{u}_{j+1}^T \mathbf{\hat{Q}_u} \mathbf{u}_{j+1}- 2u_j^T Q_{\delta u} \mathbf{E}_u \mathbf{u}_{j+1}\\
+\mathbf{x}_{j+1}^T \mathbf{\hat{Q}_x} \mathbf{x}_{j+1}- 2x_j^T Q_{\delta x} \mathbf{E}_x \mathbf{x}_{j+1}\\
+\mathbf{e}_{j+1}^T \mathbf{\hat{Q}_e} \mathbf{e}_{j+1}- 2u_j^T Q_{\delta e} \mathbf{E}_e \mathbf{e}_{j+1}\\
+\mathbf{S_x}\mathbf{x}_{j+1}
\end{equation}
where $\mathbf{\hat{Q}_u}$, $\mathbf{\hat{Q}_x}$, and $\mathbf{\hat{Q}_e}$ are defined appropriately.

Now look at the gradient of each term with respect to the elements of $\mathbf{u}_{j+1}$,
\begin{equation}
\frac{d}{d\mathbf{u}_{j+1}} \left(\mathbf{u}_{j+1}^T \mathbf{\hat{Q}_u} \mathbf{u}_{j+1} - 2u_j^T Q_{\delta u} \mathbf{E}_u \mathbf{u}_{j+1}\right)  
= 2 \mathbf{u}_{j+1}^T \mathbf{\hat{Q}_u} - 2u_j^T Q_{\delta u} \mathbf{E}_u
\end{equation}

\begin{align}
\frac{d}{d\mathbf{u}_{j+1}}\left(\mathbf{x}_{j+1}^T \mathbf{\hat{Q}_x} \mathbf{x}_{j+1} - 2x_j^T Q_{\delta x} \mathbf{E}_x \mathbf{x}_{j+1}\right)  
& = \frac{d}{d\mathbf{u}_{j+1}}\left(\left( \left(\mathbf{I}_x +\mathbf{F}_j \right)x_j + \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right) \right)^T \mathbf{\hat{Q}_x} \left( \left(\mathbf{I}_x +\mathbf{F}_j \right)x_j + \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right)\right) - 2x_j^T Q_{\delta x} \mathbf{E}_x \mathbf{x}_{j+1}\right) \\
& =2 \left( \left(\mathbf{I}_x +\mathbf{F}_j \right)x_j + \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right) \right)^T \mathbf{\hat{Q}_x}\mathbf{G}_j  - 2x_j^T Q_{\delta x} \mathbf{E}_x \mathbf{G}_j
\end{align}

\begin{align}
\frac{d}{d\mathbf{u}_{j+1}}\left(\mathbf{e}_{j+1}^T \mathbf{\hat{Q}_e} \mathbf{e}_{j+1}- 2u_j^T Q_{\delta e} \mathbf{E}_e \mathbf{e}_{j+1}\right)
& = \frac{d}{d\mathbf{u}_{j+1}} \left(\left( \mathbf{I}_e e_j -\mathbf{F}_j x_j- \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right)\right)^T \mathbf{\hat{Q}_e} \left( \mathbf{I} e_j -\mathbf{F}_j x_j - \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right) \right) - 2u_j^T Q_{\delta e} \mathbf{E}_e \mathbf{e}_{j+1} \right)\\
& = - 2  \left( \mathbf{I}_e e_j -\mathbf{F}_j x_j - \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_uu_j \right)\right)^T \mathbf{\hat{Q}_e} \mathbf{G}_j  + 2e_j^T Q_{\delta e} \mathbf{E}_e \mathbf{G}_j
\end{align}

\begin{align}
\frac{d}{d\mathbf{u}_{j+1}} \mathbf{S}_x \mathbf{x}_{j+1} 
& = \frac{d}{d\mathbf{u}_{j+1}} \mathbf{S}_x \left(\left(\mathbf{I}_x +\mathbf{F}_j \right)x_j + \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right)\right)\\
& =  \mathbf{S}_x  \mathbf{G}_j
\end{align}

So then the gradient of the performance index is

\begin{align}
\frac{d}{d\mathbf{J}_{j+1}} 
& = 2 \mathbf{u}_{j+1}^T \mathbf{\hat{Q}_u} - 2u_j^T Q_{\delta u} \mathbf{E}_u\\
& + 2 \left( \left(\mathbf{I}_x +\mathbf{F}_j \right)x_j + \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right) \right)^T \mathbf{\hat{Q}_x}\mathbf{G}_j  - 2x_j^T Q_{\delta x} \mathbf{E}_x \mathbf{G}_j\\
& - 2  \left( \mathbf{I}_e e_j -\mathbf{F}_j x_j - \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_uu_j \right)\right)^T \mathbf{\hat{Q}_e} \mathbf{G}_j  + 2e_j^T Q_{\delta e} \mathbf{E}_e \mathbf{G}_j\\
& + \mathbf{S}_x  \mathbf{G}_j
\end{align}

Setting that equal to the zero vector

\begin{align}
\vec{0}^T 
& =  \mathbf{u}_{j+1}^T \mathbf{\hat{Q}_u} - u_j^T Q_{\delta u} \mathbf{E}_u\\
& +  \left( \left(\mathbf{I}_x +\mathbf{F}_j \right)x_j + \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right) \right)^T \mathbf{\hat{Q}_x}\mathbf{G}_j  - x_j^T Q_{\delta x} \mathbf{E}_x \mathbf{G}_j\\
& -   \left( \mathbf{I}_e e_j -\mathbf{F}_j x_j - \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_uu_j \right)\right)^T \mathbf{\hat{Q}_e} \mathbf{G}_j  + e_j^T Q_{\delta e} \mathbf{E}_e \mathbf{G}_j\\
& + \frac{1}{2}\mathbf{S}_x  \mathbf{G}_j
\end{align}

Transposing the left and right hand sides
\begin{align}
\vec{0}
& =  \mathbf{\hat{Q}_u} \mathbf{u}_{j+1} -  \mathbf{E}_u^T Q_{\delta u} u_j\\
& +  \mathbf{G}_j^T \mathbf{\hat{Q}_x} \left( \left(\mathbf{I}_x +\mathbf{F}_j \right)x_j + \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_u u_j \right) \right)  -   \mathbf{G}_j^T \mathbf{E}_x^T Q_{\delta x} x_j\\
& -  \mathbf{G}_j^T \mathbf{\hat{Q}_e} \left( \mathbf{I}_e e_j -\mathbf{F}_j x_j - \mathbf{G}_j \left(\mathbf{u}_{j+1} - \mathbf{I}_uu_j \right)\right)  +   \mathbf{G}_j^T \mathbf{E}_e^T Q_{\delta e} e_j\\
& + \frac{1}{2} \mathbf{G}_j^T \mathbf{S}_x^T
\end{align}

Now multiply all the terms out
\begin{align}
\vec{0}
& =  \mathbf{\hat{Q}_u} \mathbf{u}_{j+1} -  \mathbf{E}_u^T Q_{\delta u} u_j\\
& 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{I}_x x_j 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{F}_j x_j 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{G}_j \mathbf{u}_{j+1} 
- \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{G}_j \mathbf{I}_u u_j  
- \mathbf{G}_j^T \mathbf{E}_x^T Q_{\delta x} x_j\\
& 
- \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{I}_e e_j 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{F}_j x_j 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{G}_j \mathbf{u}_{j+1} 
- \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{G}_j \mathbf{I}_u u_j 
+ \mathbf{G}_j^T \mathbf{E}_e^T Q_{\delta e} e_j\\
& + \frac{1}{2} \mathbf{G}_j^T \mathbf{S}_x^T
\end{align}


Now gather $\mathbf{u}_{j+1}$, $\mathbf{u}_{j}$, $e_j$ and $x_j$ terms.
\begin{align}
\vec{0}
& 
= \mathbf{\hat{Q}_u} \mathbf{u}_{j+1} 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{G}_j \mathbf{u}_{j+1} 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{G}_j \mathbf{u}_{j+1} \\
& 
- \mathbf{E}_u^T Q_{\delta u} u_j
- \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{G}_j \mathbf{I}_u u_j  
- \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{G}_j \mathbf{I}_u u_j \\
& 
- \mathbf{G}_j^T \mathbf{E}_x^T Q_{\delta x} x_j
+ \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{I}_x x_j 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{F}_j x_j 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{F}_j x_j\\
&
- \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{I}_e e_j 
+ \mathbf{G}_j^T \mathbf{E}_e^T Q_{\delta e} e_j\\
& + \frac{1}{2} \mathbf{G}_j^T \mathbf{S}_x^T
\end{align}
\begin{align}
\vec{0}
& 
=
\left(
\mathbf{\hat{Q}_u}
+ \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{G}_j 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{G}_j 
\right) \mathbf{u}_{j+1} \\
& 
- 
\left(
\mathbf{E}_u^T Q_{\delta u} 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{G}_j \mathbf{I}_u 
+ \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{G}_j \mathbf{I}_u 
\right) u_j \\
&
-
\left(
 \mathbf{G}_j^T \mathbf{E}_x^T Q_{\delta x} 
- \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{I}_x  
- \mathbf{G}_j^T \mathbf{\hat{Q}_x} \mathbf{F}_j  
- \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{F}_j 
\right) x_j\\
&
-
\left( \mathbf{G}_j^T \mathbf{\hat{Q}_e} \mathbf{I}_e 
- \mathbf{G}_j^T \mathbf{E}_e^T Q_{\delta e} 
\right) e_j\\
& + \frac{1}{2} \mathbf{G}_j^T \mathbf{S}_x^T
\end{align}
\begin{align}
\vec{0}
& 
=
\left(
\mathbf{\hat{Q}_u}
+ \mathbf{G}_j^T \left(\mathbf{\hat{Q}_x} + \mathbf{\hat{Q}_e} \right)\mathbf{G}_j 
\right) \mathbf{u}_{j+1} \\
& 
- 
\left(
\mathbf{E}_u^T Q_{\delta u} 
+ \mathbf{G}_j^T \left(\mathbf{\hat{Q}_x} +  \mathbf{\hat{Q}_e} \right) \mathbf{G}_j \mathbf{I}_u 
\right) u_j \\
&
- \mathbf{G}_j^T
\left(
\mathbf{E}_x^T Q_{\delta x} 
- \mathbf{\hat{Q}_x} \mathbf{I}_x  
- \mathbf{\hat{Q}_x} \mathbf{F}_j  
- \mathbf{\hat{Q}_e} \mathbf{F}_j 
\right) x_j\\
&
-\mathbf{G}_j^T
\left( \mathbf{\hat{Q}_e} \mathbf{I}_e  
- \mathbf{E}_e^T Q_{\delta e} 
\right) e_j\\
& + \frac{1}{2} \mathbf{G}_j^T \mathbf{S}_x^T
\end{align}


Solving this for $\mathbf{u}_{j+1}$ gives the optimal learning filters and the update law
\begin{align}
\mathbf{u}_{j+1} & = L_u u_j + L_e e_j + L_x x_j + L_c\\
L_0 & \triangleq \left(\mathbf{\hat{Q}_u} + \mathbf{G}_j^T \left(\mathbf{\hat{Q}_x} + \mathbf{\hat{Q}_e} \right)\mathbf{G}_j \right)^{-1}\\
L_u & \triangleq L_0\left(\mathbf{E}_u^T Q_{\delta u} + \mathbf{G}_j^T \left(\mathbf{\hat{Q}_x} +  \mathbf{\hat{Q}_e} \right) \mathbf{G}_j \mathbf{I}_u \right)\\
L_x & \triangleq L_0\mathbf{G}_j^T\left(\mathbf{E}_x^T Q_{\delta x} - \mathbf{\hat{Q}_x} \mathbf{I}_x  - \mathbf{\hat{Q}_x} \mathbf{F}_j  - \mathbf{\hat{Q}_e} \mathbf{F}_j \right)\\
L_e & \triangleq L_0 \mathbf{G}_j^T \left( \mathbf{\hat{Q}_e} \mathbf{I}_e - \mathbf{E}_e^T Q_{\delta e} \right)\\
L_c & \triangleq -\frac{1}{2}L_0\mathbf{G}_j^T\mathbf{S}_x^T
\end{align}

\end{document}















