\documentclass[landscape]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{soul}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{tabularx}
\usepackage[margin=0.25in]{geometry}
\begin{document}
\section{Introduction}
The goal of this document is to derive an ILC update law that inherently considers the impact of the next iteration, $j+1$ on the iteration after that, $j+2$.  The process will be to similar to previous work in that we will start by forming a quadratic performance index, re-write that performance index in terms of only the design variable (sequence of control inputs), differentiate that performance index with respect to the design variable, set the result equal to the zero vector, then solve or rearrange to obtain an update law.  This performance index will then include terms that depend on $u_j$, $u_{j+1}$, $u_{j+2}$, $x_j$, $x_{j+1}$, $x_{j+2}$, $e_j$, $e_{j+1}$ an $e_{j+2}$.

\section{Preliminaries}

In this work iteration $j$ is the one which was just completed, iteration $j+1$ is the iteration that is about to occur but has not yet begun.  Also, assume the path is discretized into $n_s$ steps, we have $n_x$ state variables, and $n_u$ control inputs.

Assume we are given the following a path-domain lifted system model of the form
\begin{equation}
\delta x_j = H_j \delta u_j + F_j \delta x_j^0
\end{equation}
where $\delta x_j \triangleq x_{j+1} - x_j$, $\delta u_j \triangleq u_{j+1} - u_j$ and $\delta x_j^0 \triangleq x^0_{j+1} - x^0_j$.  Here, $x^0_{j+1}$ and $x^0_j$ are the initial state vectors at iteration $j$ and iteration $j+1$ respectively.  Therefore, another expression for the lifted system model is
\begin{equation}
x_{j+1} = x_j + H_j \left(u_{j+1} - u_j\right) + F_j \left(x^0_{j+1} - x^0_j\right)
\end{equation}
  Note that for our application the final state vector from iteration $j$ is the initial state vector for iteration $j+1$.  Therefore, we define $E_F$ and $E_I$ according to
  \begin{align}
  E_F &\triangleq \begin{bmatrix} \mathbb{0}^{n_x \times n_x(n_s-1)} & \mathbb{I}^{n_x \times n_x} \end{bmatrix} \\
  E_I &\triangleq \begin{bmatrix} \mathbb{I}^{n_x \times n_x}        & \mathbb{0}^{n_x \times n_x(n_s-1)}\end{bmatrix} 
  \end{align}
  so that 
\begin{equation}
\hat{F}_j \triangleq F_j \left(E_F-E_I\right)
\end{equation}

so that the expression for the lifted system model can be written as
\begin{align}
x_{j+1}  
= & x_j + H_j \left(u_{j+1} - u_j\right) + \hat{F}_j x_j \\
= & x_j + \hat{F}_j x_j + H_j \left(u_{j+1} - u_j\right) \\
= & ( \mathbb{I} + \hat{F}_j ) x_j + H_j \left(u_{j+1} - u_j\right) \\
= & ( \mathbb{I} + \hat{F}_j ) x_j - H_j u_j + H_j u_{j+1} \\
= & x_j + \hat{F}_j x_j - H_j u_j + H_j u_{j+1} 
\end{align}

So this is our expression for $x_{j+1}$ that will be used in the performance index later on.  Next, lets find an expression for $x_{j+2}$.  If we had already run iteration $j+1$ then we could use a lifted system that looks like this

\begin{align}
x_{j+2} = & ( \mathbb{I} + F_{j+1} ) x_{j+1} + H_{j+1} \left(u_{j+2} - u_{j+1}\right) 
\end{align}
however, we cant do that.  Instead, we use the current model to approximate the behavior at iteration $j+2$, 
\begin{align}
x_{j+2} = & x_j + H_j \left(u_{j+2} - u_{j}\right) + F_j\left( x^0_{j+2}-x^0_j\right)
\end{align}

where that $x^0_{j+2}$ is the final condition from iteration $j+1$.  Therefore, noting that 
\begin{align}
x^0_{j}   &= E_I x_j \\
x^0_{j+2} &= E_F x_{j+1} \\
& = E_F \left(x_j + \hat{F}_j x_j - H_j u_j + H_j u_{j+1} \right)\\
& = E_F x_j + E_F \hat{F}_j x_j - E_F H_j u_j + E_F H_j u_{j+1}
\end{align}

makes the lifted model
\begin{align}
x_{j+2} 
= & x_j + H_j \left(u_{j+2} - u_{j}\right) + F_j\left( E_F x_j + E_F \hat{F}_j x_j - E_F H_j u_j + E_F H_j u_{j+1} - E_I x_j\right) \\
= & x_j + H_j u_{j+2} - H_j u_{j} + F_j E_F x_j + F_j E_F \hat{F}_j x_j - F_j E_F H_j u_j + F_j E_F H_j u_{j+1} - F_j E_I x_j \\
= & x_j + \left(F_j E_F + F_j E_F \hat{F}_j - F_j E_I\right)x_j - \left(F_j E_F +\mathbb{I} \right) H_j u_j + F_j E_F H_j u_{j+1}   + H_j u_{j+2}
\end{align}


Next, lets look at the expressions for error sequences, starting with $e_{j+1}$,
\begin{align}
e_{j+1} 
& = r - x_{j+1}\\
& = r - ( x_j + \hat{F}_j x_j - H_j u_j + H_j u_{j+1} ) \\
& = r -  x_j - \hat{F}_j x_j + H_j u_j - H_j u_{j+1} \\
& = e_j - \hat{F}_j x_j + H_j u_j - H_j u_{j+1} 
\end{align}

Next, the expression for $e_{j+2}$.
\begin{align}
e_{j+2} 
& = r - x_{j+2}\\
& = r - \left(x_j + \left(F_j E_F + F_j E_F \hat{F}_j - F_j E_I\right)x_j - \left(F_j E_F +\mathbb{I} \right) H_j u_j + F_j E_F H_j u_{j+1}   + H_j u_{j+2}\right) \\
& = r - x_j - \left(F_j E_F + F_j E_F \hat{F}_j - F_j E_I\right)x_j + \left(F_j E_F +\mathbb{I} \right) H_j u_j - F_j E_F H_j u_{j+1}   - H_j u_{j+2} \\
& = e_j - \left(F_j E_F + F_j E_F \hat{F}_j - F_j E_I\right)x_j + \left(F_j E_F +\mathbb{I} \right) H_j u_j - F_j E_F H_j u_{j+1}   - H_j u_{j+2} \\
\end{align}
\section{Writing the Performance Index}

So, I'm going to shoot for a super-generic performance index that penalizes the values of and iteration-to-iteration deviations in the control input, state, and error.  We also want to include a linear term on the states which can be used to incentive maximizing a given state.  So then I'll choose:
\begin{align}
J &= 
 u_{j+2}^T Q_u u_{j+2} 
 + u_{j+1}^T Q_u u_{j+1} 
 + \left(u_{j+2} - u_{j+1} \right)^T Q_{\delta u} \left(u_{j+2} - u_{j+1} \right) 
 + \left(u_{j+1} - u_{j}   \right)^T Q_{\delta u} \left(u_{j+1} - u_{j}   \right)\\
& + e_{j+2}^T Q_e e_{j+2}
  + e_{j+1}^T Q_e e_{j+1}
  + \left(e_{j+2} - e_{j+1} \right)^T Q_{\delta e} \left(e_{j+2} - e_{j+1} \right) 
  + \left(e_{j+1} - e_{j}   \right)^T Q_{\delta e} \left(e_{j+1} - e_{j}   \right)\\
& + x_{j+2}^T Q_x x_{j+2}
  + x_{j+1}^T Q_x x_{j+1}
  + \left(x_{j+2} - x_{j+1} \right)^T Q_{\delta x} \left(x_{j+2} - x_{j+1} \right) 
  + \left(x_{j+1} - x_{j} \right)^T Q_{\delta x} \left(x_{j+1} - x_{j} \right)\\
& + S_x x_{j+1} + S_x x_{j+2}
\end{align}

Start by multiplying everything out
\begin{align}
J = 
&  u_{j+2}^T Q_u u_{j+2} 
  + u_{j+1}^T Q_u u_{j+1}  
  + u_{j+2}^T Q_{\delta u} u_{j+2} - 2 u_{j+2}^T Q_{\delta u} u_{j+1} + u_{j+1}^T Q_{\delta u} u_{j+1} 
  + u_{j+1}^T Q_{\delta u} u_{j+1} - 2 u_{j+1}^T Q_{\delta u} u_{j}   +   u_{j}^T Q_{\delta u} u_{j}   \\
& + e_{j+2}^T Q_e e_{j+2} 
  + e_{j+1}^T Q_e e_{j+1}  
  + e_{j+2}^T Q_{\delta e} e_{j+2} - 2 e_{j+2}^T Q_{\delta e} e_{j+1} + e_{j+1}^T Q_{\delta e} e_{j+1} 
  + e_{j+1}^T Q_{\delta e} e_{j+1} - 2 e_{j+1}^T Q_{\delta e} e_{j}   +   e_{j}^T Q_{\delta e} e_{j}   \\
& + x_{j+2}^T Q_x x_{j+2} 
  + x_{j+1}^T Q_x x_{j+1} 
  + x_{j+2}^T Q_{\delta x} x_{j+2} - 2 x_{j+2}^T Q_{\delta x} x_{j+1} + x_{j+1}^T Q_{\delta x} x_{j+1} 
  + x_{j+1}^T Q_{\delta x} x_{j+1} - 2 x_{j+1}^T Q_{\delta x} x_{j}   +   x_{j}^T Q_{\delta x} x_{j}   \\
& + S_x x_{j+1} + S_x x_{j+2}
\end{align}

Gather like terms
\begin{align}
J =
&  u_{j+2}^T ( Q_u +   Q_{\delta u} ) u_{j+2} 
+  u_{j+1}^T ( Q_u + 2 Q_{\delta u} ) u_{j+1}  
-  2 u_{j+2}^T Q_{\delta u} u_{j+1}
-  2 u_{j+1}^T Q_{\delta u} u_{j}   +   u_{j}^T Q_{\delta u} u_{j}   \\
& +e_{j+2}^T ( Q_e +   Q_{\delta e} ) e_{j+2} 
+  e_{j+1}^T ( Q_e + 2 Q_{\delta e} ) e_{j+1}  
-  2 e_{j+2}^T Q_{\delta e} e_{j+1} 
-  2 e_{j+1}^T Q_{\delta e} e_{j}   +   e_{j}^T Q_{\delta e} e_{j}   \\
&+ x_{j+2}^T ( Q_x +   Q_{\delta x} ) x_{j+2} 
+  x_{j+1}^T ( Q_x +   Q_{\delta x} ) x_{j+1} 
-  2 x_{j+2}^T Q_{\delta x} x_{j+1} 
-  2 x_{j+1}^T Q_{\delta x} x_{j}   +   x_{j}^T Q_{\delta x} x_{j}   \\
& + S_x x_{j+1} + S_x x_{j+2}
\end{align}

So now the goal is to write expressions for the state and error trajectories over the next \textit{two} iterations in terms of the control input over the next two iterations.  To do this, define some super-lifted vectors and some block matrices
\begin{align}
\mathbf{u}_{k+1}&\triangleq \begin{bmatrix} u_{j+1} \\ u_{j+2}\end{bmatrix} \quad
\mathbf{x}_{k+1} \triangleq \begin{bmatrix} x_{j+1} \\ x_{j+2}\end{bmatrix} \quad
\mathbf{e}_{k+1} \triangleq \begin{bmatrix} e_{j+1} \\ e_{j+2}\end{bmatrix} \quad
\mathbf{r}       \triangleq \begin{bmatrix} r       \\ r      \end{bmatrix} \\
\mathbf{Q}_u    &\triangleq \begin{bmatrix} Q_u + 2 Q_{\delta u} & - Q_{\delta u} \\ - Q_{\delta u} & Q_u + Q_{\delta u} \end{bmatrix} \quad
\mathbf{Q}_e     \triangleq \begin{bmatrix} Q_e + 2 Q_{\delta e} & - Q_{\delta e} \\ - Q_{\delta e} & Q_e + Q_{\delta e} \end{bmatrix} \quad
\mathbf{Q}_x     \triangleq \begin{bmatrix} Q_u + 2 Q_{\delta x} & - Q_{\delta x} \\ - Q_{\delta x} & Q_u + Q_{\delta x} \end{bmatrix} \quad
\mathbf{S}_x     \triangleq \begin{bmatrix} S_x  &  S_x \end{bmatrix} \quad \\
\mathbf{q}_{\delta u} &\triangleq \begin{bmatrix} -2 Q_{\delta u} & \mathbb{0} \end{bmatrix} \quad
\mathbf{q}_{\delta e}  \triangleq \begin{bmatrix} -2 Q_{\delta e} & \mathbb{0} \end{bmatrix} \quad
\mathbf{q}_{\delta x}  \triangleq \begin{bmatrix} -2 Q_{\delta x} & \mathbb{0} \end{bmatrix} \quad
\end{align}
so one iteration in the $k$-domain here would include two iterations in the $j$ domain.  Now write $J$ in terms of these lifted-super vectors.  We can now use these to re-write the performance index

\begin{align}
J =
  \mathbf{u}_{k+1}^T \mathbf{Q}_u \mathbf{u}_{k+1} 
+ u_j^T \mathbf{q}_{\delta u} \mathbf{u}_{k+1}  
+ u_{j}^T Q_{\delta u} u_{j} 
+ \mathbf{e}_{k+1}^T \mathbf{Q}_e \mathbf{e}_{k+1} 
+ e_j^T \mathbf{q}_{\delta e} \mathbf{e}_{k+1}    
+ e_{j}^T Q_{\delta e} e_{j} 
+ \mathbf{x}_{k+1}^T \mathbf{Q}_x \mathbf{x}_{k+1} 
+ x_j^T \mathbf{q}_{\delta x} \mathbf{x}_{k+1}    
+ x_{j}^T Q_{\delta x} x_{j} 
+ \mathbf{S}_x \mathbf{x}_{k+1}
\end{align}

So the goal will be to differentiate this with respect to $\mathbf{u}_{k+1} $ but to do that, first we need to write $\mathbf{x}_{k+1}$ and $\mathbf{e}_{k+1}$ in terms of $\mathbf{u}_{k+1}$
\begin{align}
\mathbf{x}_{k+1} 
& = \begin{bmatrix} x_{j+1} \\ x_{j+2}\end{bmatrix} \\
& = \begin{bmatrix}  x_j + \hat{F}_j x_j - H_j u_j + H_j u_{j+1}   \\ x_j + \left(F_j E_F + F_j E_F \hat{F}_j - F_j E_I\right)x_j - \left(F_j E_F +\mathbb{I} \right) H_j u_j + F_j E_F H_j u_{j+1}   + H_j u_{j+2}\end{bmatrix} \\
& = 
  \begin{bmatrix} \mathbb{I}       \\   \mathbb{I}   \end{bmatrix}              x_j 
+ \begin{bmatrix} \hat{F}_j        \\  F_j E_F + F_j E_F \hat{F}_j - F_j E_I  \end{bmatrix}              x_j 
- \begin{bmatrix} \mathbb{I}       \\   F_j E_F + \mathbb{I}  \end{bmatrix} H_j u_j
+ \begin{bmatrix} \mathbb{I} & \mathbb{0} \\   F_j E_F  & \mathbb{I}     \end{bmatrix} H_j \mathbf{u}_{k+1} \\
& = \mathbf{I} x_j + \mathbf{F}_j x_j + \mathbf{G}_j u_j + \mathbf{H}_j \mathbf{u}_{k+1} 
\end{align}

\begin{align}
\mathbf{e}_{k+1} 
& = \begin{bmatrix} e_{j+1} \\ e_{j+2}\end{bmatrix} \\
& = \begin{bmatrix} e_j - \hat{F}_j x_j + H_j u_j - H_j u_{j+1} \\ e_j - \left(F_j E_F + F_j E_F \hat{F}_j - F_j E_I\right)x_j + \left(F_j E_F +\mathbb{I} \right) H_j u_j - F_j E_F H_j u_{j+1}   - H_j u_{j+2}\end{bmatrix} \\
& = 
\begin{bmatrix} \mathbb{I}       \\   \mathbb{I}   \end{bmatrix}              e_j 
- \begin{bmatrix} \hat{F}_j        \\  F_j E_F + F_j E_F \hat{F}_j - F_j E_I  \end{bmatrix}              x_j 
+ \begin{bmatrix} \mathbb{I}       \\   F_j E_F + \mathbb{I}  \end{bmatrix} H_j u_j
- \begin{bmatrix} \mathbb{I} & \mathbb{0} \\   F_j E_F  & \mathbb{I}     \end{bmatrix} H_j \mathbf{u}_{k+1} \\
& = \mathbf{I} e_j - \mathbf{F}_j x_j - \mathbf{G}_j u_j - \mathbf{H}_j \mathbf{u}_{k+1} 
\end{align}
with $\mathbf{M}_j$, $\mathbf{F}_j$, $\mathbf{H}_j$, and $\mathbf{I}$ defined appropriately 

These expressions mean that 
\begin{align}
\frac{d\mathbf{x}_{k+1} }{d\mathbf{u}_{k+1} } = \mathbf{H}_j \quad \mathrm{and} \quad
\frac{d\mathbf{e}_{k+1} }{d\mathbf{u}_{k+1} } = -\mathbf{H}_j
\end{align}

\section{Deriving Learning Filters}

So differentiating $J$ gives:
\begin{align}
\frac{dJ}{d\mathbf{u}_{k+1} } 
= 2 \mathbf{u}_{k+1}^T \mathbf{Q}_u 
+ u_j^T \mathbf{q}_{\delta u} 
- 2 \mathbf{e}_{k+1}^T \mathbf{Q}_e \mathbf{H}_j 
- e_j^T \mathbf{q}_{\delta e} \mathbf{H}_j
+ 2 \mathbf{x}_{k+1}^T \mathbf{Q}_x \mathbf{H}_j
+ x_j^T \mathbf{q}_{\delta x} \mathbf{H}_j  
+ \mathbf{S}_x \mathbf{H}_j
\end{align}

Transposing everything
\begin{align}
\frac{dJ}{d\mathbf{u}_{k+1} }^T 
= 2 \mathbf{Q}_u \mathbf{u}_{k+1}
+   \mathbf{q}_{\delta u}^T u_j
- 2 \mathbf{H}_j^T \mathbf{Q}_e            \mathbf{e}_{k+1}
-   \mathbf{H}_j^T \mathbf{q}_{\delta e}^T e_j
+ 2 \mathbf{H}_j^T \mathbf{Q}_x            \mathbf{x}_{k+1}
+   \mathbf{H}_j^T \mathbf{q}_{\delta x}^T x_j
+   \mathbf{H}_j^T \mathbf{S}_x^T
\end{align}

Now, sub in expressions from above for $\mathbf{x}_{k+1} $ and $\mathbf{e}_{k+1} $.
\begin{align}
\frac{dJ}{d\mathbf{u}_{k+1} }^T 
= 2 \mathbf{Q}_u \mathbf{u}_{k+1}
+   \mathbf{q}_{\delta u}^T u_j
- 2 \mathbf{H}_j^T \mathbf{Q}_e            \left(\mathbf{I} e_j - \mathbf{F}_j x_j - \mathbf{G}_j u_j - \mathbf{H}_j \mathbf{u}_{k+1}\right)
-   \mathbf{H}_j^T \mathbf{q}_{\delta e}^T e_j
+ 2 \mathbf{H}_j^T \mathbf{Q}_x            \left(\mathbf{I} x_j + \mathbf{F}_j x_j + \mathbf{G}_j u_j + \mathbf{H}_j \mathbf{u}_{k+1}\right)
+   \mathbf{H}_j^T \mathbf{q}_{\delta x}^T x_j
+   \mathbf{H}_j^T \mathbf{S}_x^T
\end{align}

Multiply everything out
\begin{align}
\frac{dJ}{d\mathbf{u}_{k+1} }^T 
=&2 \mathbf{Q}_u            \mathbf{u}_{k+1}
+   \mathbf{q}_{\delta u}^T u_j \\
&
- 2 \mathbf{H}_j^T \mathbf{Q}_e \mathbf{I} e_j 
+ 2 \mathbf{H}_j^T \mathbf{Q}_e  \mathbf{F}_j x_j 
+ 2 \mathbf{H}_j^T \mathbf{Q}_e  \mathbf{G}_j u_j 
+ 2 \mathbf{H}_j^T \mathbf{Q}_e  \mathbf{H}_j \mathbf{u}_{k+1}
-   \mathbf{H}_j^T \mathbf{q}_{\delta e}^T e_j \\
&
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{I} x_j 
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{F}_j x_j 
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{G}_j u_j 
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{H}_j \mathbf{u}_{k+1}
+   \mathbf{H}_j^T \mathbf{q}_{\delta x}^T x_j
+   \mathbf{H}_j^T \mathbf{S}_x^T
\end{align}

Gather like terms
\begin{align}
\frac{dJ}{d\mathbf{u}_{k+1} }^T 
=&2 
    \mathbf{Q}_u                              \mathbf{u}_{k+1}
+ 2 \mathbf{H}_j^T \mathbf{Q}_e  \mathbf{H}_j \mathbf{u}_{k+1}
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{H}_j   \mathbf{u}_{k+1}\\
&
+   \mathbf{q}_{\delta u}^T                   u_j 
+ 2 \mathbf{H}_j^T \mathbf{Q}_e  \mathbf{G}_j u_j 
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{G}_j   u_j \\
&
- 2 \mathbf{H}_j^T \mathbf{Q}_e  \mathbf{I}   e_j 
-   \mathbf{H}_j^T \mathbf{q}_{\delta e}^T    e_j \\
&
+ 2 \mathbf{H}_j^T \mathbf{Q}_e  \mathbf{F}_j x_j
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{I}     x_j 
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{F}_j   x_j 
+   \mathbf{H}_j^T \mathbf{q}_{\delta x}^T    x_j\\
&
+   \mathbf{H}_j^T \mathbf{S}_x^T\\
=&
\left(
2 \mathbf{Q}_u                              
+ 2 \mathbf{H}_j^T \mathbf{Q}_e  \mathbf{H}_j 
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{H}_j   
\right)\mathbf{u}_{k+1}\\
&
+\left(
    \mathbf{q}_{\delta u}^T                    
+ 2 \mathbf{H}_j^T \mathbf{Q}_e  \mathbf{G}_j  
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{G}_j   
\right) u_j\\
&
-\left(
  2 \mathbf{H}_j^T \mathbf{Q}_e  \mathbf{I}
+   \mathbf{H}_j^T \mathbf{q}_{\delta e}^T    
\right) e_j \\
&
+ \left(
  2 \mathbf{H}_j^T \mathbf{Q}_e  \mathbf{F}_j 
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{I}      
+ 2 \mathbf{H}_j^T \mathbf{Q}_x\mathbf{F}_j    
+   \mathbf{H}_j^T \mathbf{q}_{\delta x}^T    
\right) x_j \\
&
+   \mathbf{H}_j^T \mathbf{S}_x^T
\end{align}

Set it equal to zero vector and divide by 2
\begin{align}
\vec{0}
=&
\left(
   \mathbf{Q}_u                              
+  \mathbf{H}_j^T (\mathbf{Q}_e + \mathbf{Q}_x)  \mathbf{H}_j 
\right)\mathbf{u}_{k+1}\\
&
+\left(
  \frac{1}{2} \mathbf{q}_{\delta u}^T                    
+ \mathbf{H}_j^T (\mathbf{Q}_e + \mathbf{Q}_x)  \mathbf{G}_j  
\right) u_j\\
&
-\mathbf{H}_j^T\left(
    \mathbf{Q}_e  \mathbf{I}
+  \frac{1}{2} \mathbf{q}_{\delta e}^T    
\right) e_j \\
&
+ \mathbf{H}_j^T\left(
    \mathbf{Q}_e  \mathbf{F}_j 
+   \mathbf{Q}_x  \mathbf{I}      
+   \mathbf{Q}_x  \mathbf{F}_j    
+  \frac{1}{2} \mathbf{q}_{\delta x}^T    
\right) x_j \\
&
+  \frac{1}{2} \mathbf{H}_j^T \mathbf{S}_x^T
\end{align}

So solve this for $\mathbf{u}_{k+1}$
\begin{align}
\mathbf{u}_{k+1} & = \mathbf{L}_u u_j + \mathbf{L}_e e_j + \mathbf{L}_x x_j + \mathbf{L}_c \\
\mathrm{where}   & \\
\mathbf{L}_0     & \triangleq  \left(
\mathbf{Q}_u                              
+  \mathbf{H}_j^T (\mathbf{Q}_e + \mathbf{Q}_x)  \mathbf{H}_j 
\right)^{-1}\\
\mathbf{L}_u     & \triangleq -\mathbf{L}_0 \left(
\frac{1}{2} \mathbf{q}_{\delta u}^T                    
+ \mathbf{H}_j^T (\mathbf{Q}_e + \mathbf{Q}_x)  \mathbf{G}_j  
\right)\\
\mathbf{L}_e     & \triangleq  \mathbf{L}_0 \mathbf{H}_j^T\left(
\mathbf{Q}_e  \mathbf{I}
+  \frac{1}{2} \mathbf{q}_{\delta e}^T    
\right)\\
\mathbf{L}_x     & \triangleq -\mathbf{L}_0 \mathbf{H}_j^T\left(
\mathbf{Q}_e  \mathbf{F}_j 
+   \mathbf{Q}_x  \mathbf{I}      
+   \mathbf{Q}_x  \mathbf{F}_j    
+  \frac{1}{2} \mathbf{q}_{\delta x}^T    
\right)\\
\mathbf{L}_c     & \triangleq -\mathbf{L}_0 \frac{1}{2} \mathbf{H}_j^T \mathbf{S}_x^T
\end{align}

\end{document}















